<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ashwinee Panda</title>

    <meta name="author" content="Ashwinee Panda">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
    <style>
      .selectedButton {
        background-color: #4CAF50;
        color: white;
      }
    </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ashwinee Panda
                </p>
                <p>
                  I am a Postdoctoral Fellow at <a href="https://www.trails.umd.edu/">UMD</a> working with <a href="https://twitter.com/tomgoldsteincs">Tom Goldstein</a> and his amazing students on large language models. My research is generously supported by a series of grants from Open Philanthropy.</p>
                  <p>
                  I received my PhD from <a href="https://princeton.edu/">Princeton University</a> working with <a href="https://www.princeton.edu/~pmittal/">Prateek Mittal</a>. 
                During my PhD, I received <a href="https://openai.com/blog/superalignment-fast-grants">the OpenAI Superalignment Fast Grant</a> for our work showing that <a href="https://openreview.net/forum?id=6Mxhg9PtDE">current safety alignment is shallow</a>, which received <a href="https://blog.iclr.cc/2025/04/22/announcing-the-outstanding-paper-awards-at-iclr-2025/">the Outstanding Paper Award at ICLR 2025</a>.
                
                <p>
                  <b>I am currently on the job market.</b>
                </p>
                <p style="text-align:center">
                  <a href="data/AshwineePandaCV_2025.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=rFC3l6YAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/PandaAshwinee">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/kiddyboots216">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profpic_square.jpg"><img style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo" src="images/profpic_square.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <div style="max-height:180px;overflow-y:auto;padding-right:8px;">
                  <ul style="margin:0 0 0 20px;">
                    <li><strong>Jan 2026:</strong> 2 papers accepted to ICLR 2026: <a href="https://arxiv.org/abs/2509.02563">DynaGuard</a> and <a href="https://arxiv.org/abs/2504.03889">Attention Pruning</a>.</li>
                    <li><strong>Nov 2025:</strong> I received a $310,000 grant from Open Philanthropy to build pretraining datasets.</li>
                    <li><strong>Oct 2025:</strong> I received a $310,000 grant from Open Philanthropy to build reasoning models.</li>
                    <li><strong>Sep 2025:</strong> 3 papers accepted to NeurIPS 2025: <a href="https://arxiv.org/abs/2504.12463">Dense Backprop</a>, <a href="https://arxiv.org/abs/2502.06857">Gemstones</a>, and <a href="https://finegrainbench.ai/">FineGRAIN</a>.</li>
                    <li><strong>Jul 2025:</strong> 2 papers accepted to COLM 2025: <a href="https://arxiv.org/abs/2412.06748">Refusal Tokens</a> and <a href="https://arxiv.org/abs/2504.07448">LoRI</a>.</li>
                    <li><strong>Jun 2025:</strong> I received a $218,000 grant from Open Philanthropy to study encoded reasoning.</li>
                    <li><strong>Apr 2025:</strong> ICLR 2025 Outstanding Paper Award for <a href="https://openreview.net/forum?id=6Mxhg9PtDE">Safety Alignment</a>.</li>
                    <li><strong>Apr 2025:</strong> I received a $150,000 grant from Longview to study the construction of pretraining datasets.</li>
                  </ul>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  <p>The goal of my research is to build useful systems. To me, a useful system (1) does what you want it to do, (2) is personalized to you, and (3) can be developed and deployed by you. These desiredata inform the three focuses of my research agenda: safety (the ability to control the behavior of the system), privacy (the ability to personalize the system to you), and efficiency (the ability to develop and deploy the system quickly and cheaply).</p>

                  See below for selected publications.
                </p>
              </td>
            </tr>
            <tr>
              <td style="text-align:left;padding-left:20px;">
                <button id="safetyButton" class="selectedButton" onclick="filterPapers('safety'); highlightButton('safetyButton')">Safety</button>
                <button id="privacyButton" onclick="filterPapers('privacy'); highlightButton('privacyButton')">Privacy</button>
                <button id="efficiencyButton" onclick="filterPapers('efficiency'); highlightButton('efficiencyButton')">Efficiency</button>
                <script>
                  function highlightButton(buttonId) {
                    document.getElementById('safetyButton').classList.remove('selectedButton');
                    document.getElementById('privacyButton').classList.remove('selectedButton');
                    document.getElementById('efficiencyButton').classList.remove('selectedButton');
                    document.getElementById(buttonId).classList.add('selectedButton');
                  }
                  function filterPapers(category) {
                    var papers = document.querySelectorAll('.paper');
                    papers.forEach(function(paper) {
                      if (paper.getAttribute('data-category') === category) {
                        paper.style.display = 'table-row';
                      } else {
                        paper.style.display = 'none';
                      }
                    });
                    // Update category description text
                    document.getElementById('safety-desc').style.display = (category === 'safety') ? 'block' : 'none';
                    document.getElementById('privacy-desc').style.display = (category === 'privacy') ? 'block' : 'none';
                    document.getElementById('efficiency-desc').style.display = (category === 'efficiency') ? 'block' : 'none';
                  }
                  document.addEventListener('DOMContentLoaded', function() {
                    highlightButton('safetyButton');
                    filterPapers('safety');
                  }, false);
                </script>
              </td>
            </tr>
          </tbody></table>

          <div style="padding:0 20px 10px 20px;">
            <p id="safety-desc" style="display:block;">My research in safety focuses on
               understanding how users can control the behavior of systems. 
               In <i>Shallow Alignment</i>, 
               we show how the methods by which users leverage control over their systems 
               -prompting, prefilling, modifying sampling parameters, and finetuning the model- 
               can easily remove the alignment of the system. 
               In <i>Refusal Tokens</i>,
                we show how to calibrate multiple kinds of refusal messages. 
                In <i>DynaGuard</i>,
                we show how to to redefine safety as a dynamic process that can be controlled by users.
                </p>
            <p id="privacy-desc" style="display:none;">My research in privacy is focused toward 
              the goal of being able to personalize the system to user data, a primary obstacle
              to which is potential privacy violations. The two sides to my work are attacks and defenses.
              In <i>Neural Phishing</i> and <i>Privacy Auditing</i>, we develop new attacks to upper bound 
              how much information can be extracted from the system. In the rest of my work, we develop 
              efficient methods for adapting models to user data with differential privacy.
            </p>
            <p id="efficiency-desc" style="display:none;">My research in efficiency focuses on 
              building systems that can be developed and deployed by users. In <i>LoTA</i> and <i>LoRI</i>, 
              we work on parameter-efficient methods for adapting models to user data. In <i>Gemstones</i> and 
              <i>Dense Backprop</i>, we investigate how to shape models for efficient pretraining.
            </p>
          </div>

          <div id="papersList">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- SAFETY PAPERS -->
            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/shallow_alignment.png" alt="deep alignment" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/forum?id=6Mxhg9PtDE">
                  <span class="papertitle">Safety Alignment Should be Made More Than Just a Few Tokens Deep
                  </span>
                </a>
                <br>
                Xiangyu Qi,
                <strong>Ashwinee Panda</strong>,
                Kaifeng Lyu,
                Xiao Ma,
                Subhrajit Roy,
                Ahmad Beirami,
                Prateek Mittal,
                Peter Henderson
                <br>
                At <em>ICLR 2025 (Outstanding Paper Award)</em>
                <br>
                <a href="https://openreview.net/forum?id=6Mxhg9PtDE">paper</a> /
                <a href="https://github.com/Unispac/shallow-vs-deep-alignment">code</a>
                <p>We analyze safety alignment and show that it is largely shallow. We propose methods for making it deeper.</p>
              </td>
            </tr>

            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/refusaltokens.jpg" alt="refusal tokens" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2412.06748">
                  <span class="papertitle">Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</span>
                </a>
                <br>
                Neel Jain,
                Aditya Shrivastava,
                Chenyang Zhu,
                Daben Liu,
                Alfy Samuel,
                <strong>Ashwinee Panda</strong>,
                Anoop Kumar,
                Micah Goldblum,
                Tom Goldstein
                <br>
                At <em>COLM 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2412.06748">paper</a> /
                <a href="https://huggingface.co/collections/tomg-group-umd/refusal-token-models">models</a>
                <p>Refusal tokens enable controlling a single model's refusal rates without further fine-tuning, by selectively intervening during generation.</p>
              </td>
            </tr>

            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dynaguard.jpg" alt="dynaguard" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2509.02563">
                  <span class="papertitle">DynaGuard: A Dynamic Guardian Model With User-Defined Policies</span>
                </a>
                <br>
                Monte Hoover,
                Vatsal Baherwani,
                Neel Jain,
                Khalid Saifullah,
                Joseph Vincent,
                Chirag Jain,
                Melissa Kazemi Rad,
                C. Bayan Bruss,
                <strong>Ashwinee Panda</strong>,
                Tom Goldstein
                <br>
                <em>At ICLR 2026</em>
                <br>
                <a href="https://arxiv.org/abs/2509.02563">paper</a> /
                <a href="https://huggingface.co/collections/tomg-group-umd/dynaguard">models</a>
                <p>DynaGuard is a dynamic guardian model that evaluates text based on user-defined policies, surpassing static models in detection accuracy.</p>
              </td>
            </tr>

            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/vlm.jpg" alt="VLM" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://twitter.com/PandaAshwinee/status/1673948702397046786">
                  <span class="papertitle">Visual Adversarial Examples Jailbreak Aligned Large Language Models
                  </span>
                </a>
                <br>
                Xiangyu Qi*,
                Kaixuan Huang*,
                <strong>Ashwinee Panda</strong>,
                Peter Henderson,
                Mengdi Wang,
                Prateek Mittal
                <br>
                At <em>AAAI 2024 (Oral)</em>
                <br>
                <a href="https://arxiv.org/abs/2306.13213">paper</a> /
                <a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models">code</a>
                <p>We propose the first method for generating visual adversarial examples that can serve as transferrable universal jailbreaks against aligned large language models.</p>
              </td>
            </tr>

            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/neurotoxin.jpg" alt="neurotoxin" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://proceedings.mlr.press/v162/zhang22w">
                  <span class="papertitle">Neurotoxin: Durable Backdoors in Federated Learning</span>
                </a>
                <br>
                Zhengming Zhang*,
                <strong>Ashwinee Panda*</strong>,
                Linyue Song,
                Yaoqing Yang,
                Prateek Mittal,
                Joseph Gonzalez,
                Kannan Ramchandran,
                Michael Mahoney
                <br>
                In <em>ICML 2022 (Spotlight)</em>
                <br>
                <a href="https://arxiv.org/abs/2206.10341">paper</a> /
                <a href="https://drive.google.com/file/d/1sDu_5xWNzU20dleDJWC5JThXtn5BLJLL/view?usp=sharing">poster</a> /
                <a href="https://github.com/jhcknzzm/Federated-Learning-Backdoor">code</a>
                <p>Neurotoxin is a novel model poisoning attack for federated learning that stays present in the system for up to 5X longer than the baseline attack.</p>
              </td>
            </tr>

            <tr class="paper" data-category="safety">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/sparsefed.jpg" alt="sparsefed" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://proceedings.mlr.press/v151/panda22a.html">
                  <span class="papertitle">SparseFed: Mitigating Model Poisoning Attacks in Federated Learning via Sparsification</span>
                </a>
                <br>
                <strong>Ashwinee Panda</strong>,
                Saeed Mahloujifar,
                Arjun Bhagoji,
                Supriyo Chakraborty,
                Prateek Mittal
                <br>
                In <em>AISTATS 2022</em>
                <br>
                <a href="https://arxiv.org/abs/2112.06274">paper</a> /
                <a href="https://github.com/kiddyboots216/CommEfficient/tree/attacks">code</a>
                <p>SparseFed is a provably robust defense against model poisoning attacks in federated learning that uses server-side sparsification to avoid updating malicious neurons.</p>
              </td>
            </tr>

            

            <!-- PRIVACY PAPERS -->
            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/auditing.png" alt="privacy auditing" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2503.06808">
                  <span class="papertitle">Privacy Auditing of Large Language Models
                  </span>
                </a>
                <br>
                <strong>Ashwinee Panda*</strong>,
                Xinyu Tang*,
                Milad Nasr,
                Christopher A. Choquette-Choo,
                Prateek Mittal
                <br>
                At <em>ICLR 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2503.06808">paper</a> /
                <a href="https://x.com/PandaAshwinee/status/1900182427651248559">thread</a>
                <p>We present the first method for doing privacy auditing of LLMs.</p>
              </td>
            </tr>

            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/phishingfigure.png" alt="gpt phish" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://twitter.com/PandaAshwinee/status/1749190096987853023">
                  <span class="papertitle">Teach LLMs to Phish: Stealing Private Information from Language Models
                  </span>
                </a>
                <br>
                <strong>Ashwinee Panda</strong>,
                Christopher A. Choquette-Choo,
                Zhengming Zhang,
                Yaoqing Yang,
                Prateek Mittal
                <br>
                At <em>ICLR 2024</em>
                <br>
                <a href="https://www.youtube.com/watch?v=Tyw51pIp_gg&list=PLSIUOFhnxEiDoTNvhZWIm1PNBAFJWUxU8&index=2">talk</a> /
                <a href="https://arxiv.org/abs/2403.00871">paper</a> /
                <a href="https://x.com/PandaAshwinee/status/1749190096987853023">thread</a>
                <p>We propose a new practical data extraction attack that we call "neural phishing". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data.</p>
              </td>
            </tr>

            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dpzo.png" alt="dp zo" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2401.04343">
                  <span class="papertitle">Private Fine-tuning of Large Language Models with Zeroth-order Optimization
                  </span>
                </a>
                <br>
                Xinyu Tang*,
                <strong>Ashwinee Panda*</strong>,
                Milad Nasr*,
                Saeed Mahloujifar,
                Prateek Mittal
                <br>
                At <em>TMLR 2025, TPDP 2024 (Oral)</em>
                <br>
                <a href="https://arxiv.org/abs/2401.04343">paper</a>
                <p>We propose the first method for performing differentially private fine-tuning of large language models without backpropagation. Our method is the first to provide a nontrivial privacy-utility tradeoff under pure differential privacy.</p>
              </td>
            </tr>

            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Pipeline.png" alt="dp icl" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://twitter.com/PandaAshwinee/status/1747655539168059464">
                  <span class="papertitle">Privacy-Preserving In-Context Learning for Large Language Models</span>
                </a>
                <br>
                Tong Wu*,
                <strong>Ashwinee Panda*</strong>,
                Tianhao Wang*,
                Prateek Mittal
                <br>
                At <em>ICLR 2024</em>
                <br>
                <a href="https://www.youtube.com/watch?v=Tyw51pIp_gg&list=PLSIUOFhnxEiDoTNvhZWIm1PNBAFJWUxU8&index=2">talk</a> /
                <a href="https://arxiv.org/abs/2305.01639">paper</a> /
                <a href="https://github.com/tongwu2020/DP-ICL">code</a> /
                <a href="https://x.com/PandaAshwinee/status/1747655539168059464">thread</a>
                <p>We propose the first method for performing differentially private in-context learning. Our method generates sentences from in-context learning while keeping the in-context exemplars differentially private, that can be applied to blackbox APIs (ex RAG).</p>
              </td>
            </tr>

            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dplinearscaling.png" alt="linear scaling" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2212.04486">
                  <span class="papertitle">A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization
                  </span>
                </a>
                <br>
                <strong>Ashwinee Panda*</strong>,
                Xinyu Tang*,
                Vikash Sehwag,
                Saeed Mahloujifar,
                Prateek Mittal
                <br>
                At <em>ICML 2024</em>
                <br>
                <a href="https://www.youtube.com/watch?v=KzxAP20TMJc&list=PLSIUOFhnxEiDoTNvhZWIm1PNBAFJWUxU8&index=6">talk</a> /
                <a href="https://arxiv.org/abs/2212.04486">paper</a> /
                <a href="https://github.com/kiddyboots216/dp-custom/">code</a>
                <p>We find that using scaling laws for Differentially Private Hyperparameter Optimization significantly outperforms prior work in privacy and compute cost.</p>
              </td>
            </tr>
            
            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dprandp.jpg" alt="dp random priors" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://twitter.com/PandaAshwinee/status/1668343271691816962">
                  <span class="papertitle">Differentially Private Image Classification by Learning Priors from Random Processes</span>
                </a>
                <br>
                Xinyu Tang*,
                <strong>Ashwinee Panda*</strong>,
                Vikash Sehwag,
                Prateek Mittal
                <br>
                At <em>NeurIPS 2023 (Spotlight)</em>
                <br>
                <a href="https://arxiv.org/abs/2306.06076">paper</a> /
                <a href="https://github.com/inspire-group/DP-RandP">code</a>
                <p>We pretrain networks with synthetic images that have strong performance on downstream private computer vision tasks.</p>
              </td>
            </tr>

            <tr class="paper" data-category="privacy">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dpdiffusion.jpg" alt="dp diffusion" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1s5wI9A-z4OBmucL5IVVFwx9hCZqfYsWX/view?usp=sharing">
                  <span class="papertitle">Differentially Private Generation of High Fidelity Samples From Diffusion Models</span>
                </a>
                <br>
                Vikash Sehwag*,
                <strong>Ashwinee Panda*</strong>,
                Ashwini Pokle,
                Xinyu Tang,
                Saeed Mahloujifar,
                Mung Chiang,
                J Zico Kolter,
                Prateek Mittal
                <br>
                At <em>ICML 2023</em> GenAI Workshop
                <br>
                <a href="https://openreview.net/pdf?id=vuVGcl0ed1">paper</a> /
                <a href="https://drive.google.com/file/d/1s5wI9A-z4OBmucL5IVVFwx9hCZqfYsWX/view?usp=sharing">poster</a>
                <p>We generate differentially private images from non-privately trained diffusion models by analyzing the inherent privacy of stochastic sampling.</p>
              </td>
            </tr>

            <!-- EFFICIENCY PAPERS -->
            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/expertapprox.png" alt="Dense Backpropagation" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.12463">
                  <span class="papertitle">Dense Backpropagation Improves Training for Sparse Mixture-of-Experts
                  </span>
                </a>
                <br>
                <strong>Ashwinee Panda</strong>,
                Vatsal Baherwani,
                Zain Sarwar,
                Benjamin Therien,
                Supriyo Chakraborty,
                Tom Goldstein
                <br>
                At <em>NeurIPS 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2504.12463">paper</a> /
                <a href="https://github.com/vatsal0/default-moe">code</a> /
                <a href="https://x.com/PandaAshwinee/status/1914124872802091428">thread</a>
                <p>We present a lightweight approximation method that gives the MoE router a dense gradient update while continuing to sparsely activate its parameters.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gemstones_teaser.png" alt="gemstones" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2502.06857">
                  <span class="papertitle">Gemstones üíé: A Model Suite for Multi-Faceted Scaling Laws
                  </span>
                </a>
                <br>
                Sean McLeish,
                John Kirchenbauer,
                David Yu Miller,
                Siddharth Singh,
                Abhinav Bhatele,
                Micah Goldblum,
                <strong>Ashwinee Panda‚ô†Ô∏è</strong>,
                Tom Goldstein
                <br>
                At <em>NeurIPS 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2502.06857">paper</a> /
                <a href="https://github.com/mcleish7/gemstone-scaling-laws">code</a> /
                <a href="https://huggingface.co/collections/tomg-group-umd/gemstone-models-679408ee3f19f1d4d00e8b10">models</a> /
                <a href="https://mcleish7.github.io/gemstone-scaling-laws/">website</a> /
                <a href="https://x.com/PandaAshwinee/status/1897361636379533353">thread</a>
                <p>We release the Gemstone model suite for open-source scaling laws.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/lori.png.jpg" alt="LoRI" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.07448">
                  <span class="papertitle">LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation
                  </span>
                </a>
                <br>
                Juzheng Zhang,
                Jiacheng You,
                <strong>Ashwinee Panda</strong>,
                Tom Goldstein
                <br>
                At <em>COLM 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2504.07448">paper</a> /
                <a href="https://huggingface.co/collections/tomg-group-umd/lori-adapters">models</a>
                <p>LoRI reduces cross-task interference in multi-task low-rank adaptation, enabling better performance when fine-tuning on multiple tasks.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/vdit.png" alt="Video Diffusion Attention" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.10317">
                  <span class="papertitle">Analysis of Attention in Video Diffusion Transformers
                  </span>
                </a>
                <br>
                Yuxin Wen,
                Jim Wu,
                Ajay Jain,
                Tom Goldstein,
                <strong>Ashwinee Panda</strong>
                <br>
                <em>Arxiv 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2504.10317">paper</a> /
                <a href="https://tinyurl.com/vditattention">website</a> /
                <a href="https://x.com/PandaAshwinee/status/1912523804540100677">thread</a>
                <p>We conduct an in-depth analysis of attention in video diffusion transformers (VDiTs) and report a number of novel findings.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/attentionsink.jpeg" alt="Attention Sink Dormant Heads" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.03889">
                  <span class="papertitle">Identifying and Evaluating Inactive Heads in Pretrained LLMs
                  </span>
                </a>
                <br>
                Pedro Sandoval-Segura,
                Xijun Wang,
                <strong>Ashwinee Panda</strong>,
                Micah Goldblum,
                Ronen Basri,
                Tom Goldstein,
                David Jacobs
                <br>
                <em>At ICLR 2026</em>
                <br>
                <a href="https://arxiv.org/abs/2504.03889">paper</a> /
                <a href="https://x.com/psandovalsegura/status/1909652533334712691">thread</a>
                <p>We propose a new definition for attention heads dominated by attention sinks, known as dormant attention heads.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/lota.png" alt="lota" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2406.16797">
                  <span class="papertitle">Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs
                  </span>
                </a>
                <br>
                <strong>Ashwinee Panda</strong>,
                Berivan Isik,
                Xiangyu Qi,
                Sanmi Koyejo,
                Tsachy Weissman,
                Prateek Mittal
                <br>
                At <em>ICML 2024 WANT (Best Paper) / ES-FoMO (Oral) </em>
                <br>
                <a href="https://arxiv.org/abs/2401.04343">paper</a> /
                <a href="https://github.com/kiddyboots216/lottery-ticket-adaptation/">code</a> /
                <a href="https://x.com/PandaAshwinee/status/1825571610230723027">thread</a>
                <p>Lottery Ticket Adaptation (LoTA) is a new adaptation method that handles challenging tasks, mitigates catastrophic forgetting, and enables model merging across different tasks.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/fetchsgd.jpg" alt="fetchsgd" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://proceedings.mlr.press/v119/rothchild20a.html">
                  <span class="papertitle">FetchSGD: Communication-Efficient Federated Learning with Sketching</span>
                </a>
                <br>
                Daniel Rothchild*,
                <strong>Ashwinee Panda*</strong>,
                Enayat Ullah,
                Nikita Ivkin,
                Ion Stoica,
                Vladimir Braverman,
                Joseph Gonzalez,
                Raman Arora
                <br>
                In <em>ICML 2020</em>
                <br>
                <a href="https://arxiv.org/abs/2007.07682">paper</a> /
                <a href="https://github.com/kiddyboots216/CommEfficient">code</a>
                <p>FetchSGD is a communication-efficient federated learning algorithm that compresses gradient updates with sketches.</p>
              </td>
            </tr>

            <tr class="paper" data-category="efficiency">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/softpbt.jpg" alt="softpbt" width="160" style="border-style: none">             
              </td>
                <td width="75%" valign="middle">
                  <a href="https://github.com/kiddyboots216/SoftPBT/blob/master/ICLR_2020___SoftPBT%20(2).pdf">
                    <span class="papertitle">SoftPBT: Leveraging Experience Replay for Efficient Hyperparameter Schedule Search</span>
                  </a>
                  <br>
                  <strong>Ashwinee Panda</strong>,
                  Eric Liang,
                  Richard Liaw,
                  Joey Gonzalez
                  <br>
                  <a href="https://github.com/kiddyboots216/SoftPBT/blob/master/ICLR_2020___SoftPBT%20(2).pdf">paper</a> /
                  <a href="https://github.com/kiddyboots216/SoftPBT">code</a>
              </td>
            </tr>
              
          </tbody></table>
          </div>
              
                
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Not Research</h2>
                <p style="text-align:center">
                  <a href="data/AshwineePanda-bio.txt">WeChat</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/ashwineepanda/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://www.instagram.com/ashwineepanda/">Instagram</a> &nbsp;/&nbsp;
                  <a href="https://www.yelp.com/user_details_reviews_self?userid=X1HlGDvL3RElFC9JiWQpyA">Yelp</a> &nbsp;/&nbsp;
                  <a href="https://www.goodreads.com/user/show/18314561-ashwinee">Goodreads</a>&nbsp;/&nbsp;
                  <a href="https://open.spotify.com/playlist/4z3RNRCOyFHHyfKi98LCYE?si=1a05c22007814de6">Spotify</a>
                </p>
                <p>
                  <a href="https://en.wikipedia.org/wiki/San_Jose,_California">I've</a> 
                  <a href="https://en.wikipedia.org/wiki/Berkeley,_California">lived</a> 
                  <a href="https://en.wikipedia.org/wiki/Princeton,_New_Jersey">all</a> 
                  <a href="https://en.wikipedia.org/wiki/New_York_City">over</a> 
                  <a href="https://en.wikipedia.org/wiki/Washington,_D.C.">America</a>
                  and traveled across the world.
                  Feel free to ask me for food recs in <a href="https://www.instagram.com/stories/highlights/18124010734224361/">NYC</a>, <a href="https://www.instagram.com/stories/highlights/17920690429823023/">LA</a>, the SF Bay Area, or the DMV. 
                </p>
                <p>
                  While at Berkeley I founded <a href="https://discreetai.com/">DiscreetAI</a> (now inactive), a <a href="https://www.samsungnext.com/">venture-backed</a> startup building <a href="https://github.com/DiscreetAI/decentralized-ml">privacy-preserving machine learning as-a-service</a>. You can check out <a href="https://www.producthunt.com/products/npm#discreetai">our ProductHunt launch</a> or <a href="https://github.com/discreetai">our GitHub</a> for more information. Among <a href="https://newsroom.haas.berkeley.edu/startups-pitch-win-2018-launch-finals/">other things</a> we won the <a href="https://www.instagram.com/p/BhvgMoDBh6L/?img_index=3">first YCombinator Hackathon</a> and built <a href="https://github.com/DiscreetAI/discreetai-chatbot">federated learning solutions</a> for <a href="https://www.ford.com/">Fortune 500 companies</a>.
                </p>
              
                <p>
                I read a lot and post my thoughts (mostly now on Twitter, previously on Goodreads). 
                <li><a href="https://x.com/PandaAshwinee/status/1874720150953562348">2025; Twitter thread of 60 books read.</a></li>
                <li><a href="https://x.com/PandaAshwinee/status/1759441743026491710">2024; Twitter thread of 23 books read.</a></li> 
                <li><a href="https://x.com/PandaAshwinee/status/1739934767720657222">2023; ELO rating tierlist (72 books).</a></li> 
                <li><a href="https://www.goodreads.com/user/year_in_books/2021/18314561">2021; Goodreads year in review (100 books).</a></li>
              </p>
              </td>
                </tr>
              </tbody></table>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:small;">
                      Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                    </p>
                  </td>
                </tr>
              </tbody></table>
            </td>
          </tr>
        </table>
      </body>
    </html>
